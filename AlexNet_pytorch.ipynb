{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNy4S3Ycn4h1ttgXJhsAj36",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masalha-alaa/alexnet-pytorch/blob/master/AlexNet_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR7PwJYcXSFZ"
      },
      "source": [
        "# AlexNet Neural Network\n",
        "This is an implementation of the famous AlexNet Neural Net (the winner of ImageNet 2012) which was described in the paper:\n",
        "https://www.cs.toronto.edu/~hinton/absps/imagenet.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4FYj5nNXTCz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enCaOTm-Xjkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159add34-cfa8-4da7-dc77-19cef704dfa1"
      },
      "source": [
        "NUM_CLASSES = 1000  # Number of categories (classes) in ImageNet\n",
        "IMAGE_SHAPE = (3, 227, 227)  # there's a mistake in the paper. the cropping is 227 not 224.\n",
        "                             # See: https://datascience.stackexchange.com/questions/29245/what-is-the-input-size-of-alex-net\n",
        "EPOCHS = 90\n",
        "\n",
        "\n",
        "def calc_out_size(in_size, padding, kernel, stride):\n",
        "    # formula from: https://youtu.be/wnK3uWv_WkU?t=234\n",
        "    # further reading: https://blog.mlreview.com/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807\n",
        "    # assuming symmetric nn\n",
        "    padding = padding if isinstance(padding, int) else padding[0]\n",
        "    kernel = kernel if isinstance(kernel, int) else kernel[0]\n",
        "    stride = stride if isinstance(stride, int) else stride[0]\n",
        "    return ((in_size + 2*padding - kernel) // stride) + 1\n",
        "\n",
        "\n",
        "def same_padding(in_out_size, kernel_size, stride):\n",
        "    \"\"\"\n",
        "    A pading metric called \"Same\", which doesn't exist in PyTorch. It exists in Keras though.\n",
        "    I figured out the formula simply by extracting the \"padding\" variable from the formula used in calc_out_size():\n",
        "    out = [(in + 2*padding - kernel) / stride] + 1\n",
        "    where in == out\n",
        "    padding = (out * stride - out + kernel - stride) // 2\n",
        "    \"\"\"\n",
        "    return (in_out_size * stride - in_out_size + kernel_size - stride * 1) // 2\n",
        "\n",
        "\n",
        "def weights_init(m, bias=1.0):\n",
        "    # Gaussian Weight initalization before each layer: mean 0: std: 0.01.\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.01)\n",
        "    if bias:\n",
        "        m.bias.data.fill_(bias)\n",
        "\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, dimensions, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.dimensions = dimensions\n",
        "        self.num_classes = num_classes\n",
        "        self.current_size = 0\n",
        "        \n",
        "        # Note: In layers 2,3,4,5 I use \"Same\" padding. This is a padding metric that keeps the layer's output size the same as the\n",
        "        # previous layer, by sitting the appropriate padding size.\n",
        "        # I use this because in the paper they don't mention the padding they picked, but one can conclude it from the layers sizes in\n",
        "        # the snapshot in the paper: 55x55 => 27x27 => 13x13 => 13x13 => 13x13.\n",
        "        # Also Andrew NG. says that \"Same\" padding is used in these layers. Watch:\n",
        "        # https://www.coursera.org/lecture/convolutional-neural-networks/classic-networks-MmYe2\n",
        "        # 8:04, 8:20+\n",
        "        \n",
        "        conv_stride = 4  # common for all convolution layers\n",
        "\n",
        "        # layer 1\n",
        "        layer_kernel_size = 11\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=96, kernel_size=layer_kernel_size, stride=conv_stride,\n",
        "                                             bias=False),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2.0),\n",
        "                                   nn.MaxPool2d(kernel_size=3, stride=2))\n",
        "        weights_init(self.conv1[0], bias=False)\n",
        "        self.update_size(self.conv1[0], dims=self.dimensions)\n",
        "        self.update_size(self.conv1[3])  # pool\n",
        "        \n",
        "        # layer 2\n",
        "        layer_kernel_size = 5\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=self.conv1[0].out_channels, out_channels=256, kernel_size=layer_kernel_size, stride=conv_stride,\n",
        "                                             padding=same_padding(self.current_size, kernel_size=layer_kernel_size, stride=conv_stride)),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2.0),\n",
        "                                   nn.MaxPool2d(kernel_size=3, stride=2))\n",
        "        weights_init(self.conv2[0])\n",
        "        self.update_size(self.conv2[0])\n",
        "        self.update_size(self.conv2[3])  # pool\n",
        "        \n",
        "        # layer 3\n",
        "        layer_kernel_size = 3\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_channels=self.conv2[0].out_channels, out_channels=384, kernel_size=layer_kernel_size, stride=conv_stride,\n",
        "                                             padding=same_padding(self.current_size, kernel_size=layer_kernel_size, stride=conv_stride),\n",
        "                                             bias=False),\n",
        "                                   nn.ReLU())\n",
        "        weights_init(self.conv3[0], bias=False)\n",
        "        self.update_size(self.conv3[0])\n",
        "        \n",
        "        # layer 4\n",
        "        layer_kernel_size = 3\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_channels=self.conv3[0].out_channels, out_channels=384, kernel_size=layer_kernel_size, stride=conv_stride,\n",
        "                                             padding=same_padding(self.current_size, kernel_size=layer_kernel_size, stride=conv_stride)),\n",
        "                                   nn.ReLU())\n",
        "        weights_init(self.conv4[0])\n",
        "        self.update_size(self.conv4[0])\n",
        "    \n",
        "        # layer 5\n",
        "        layer_kernel_size = 3\n",
        "        self.conv5 = nn.Sequential(nn.Conv2d(in_channels=self.conv4[0].out_channels, out_channels=256, kernel_size=layer_kernel_size, stride=conv_stride,\n",
        "                                             padding=same_padding(self.current_size, kernel_size=layer_kernel_size, stride=conv_stride)),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.MaxPool2d(kernel_size=3, stride=2))\n",
        "        weights_init(self.conv5[0])\n",
        "        self.update_size(self.conv5[0])\n",
        "        self.update_size(self.conv5[2])  # pool\n",
        "\n",
        "        # Note: Not clear whether Dropout goes before or after in the following 2 layers. To my understanding it's after.\n",
        "        # See: https://stats.stackexchange.com/questions/240305/where-should-i-place-dropout-layers-in-a-neural-network\n",
        "\n",
        "        # layer 6\n",
        "        self.fc1 = nn.Sequential(nn.Linear(self.conv5[0].out_channels*self.current_size*self.current_size, 4096),\n",
        "                                 nn.Dropout(0.5))\n",
        "        weights_init(self.fc1[0])\n",
        "        \n",
        "        # layer 7\n",
        "        self.fc2 = nn.Sequential(nn.Linear(self.fc1[0].out_features, 4096),\n",
        "                                 nn.Dropout(0.5))\n",
        "        weights_init(self.fc2[0])\n",
        "        \n",
        "        # output layer (softmax)\n",
        "        self.fc3 = nn.Sequential(nn.Linear(self.fc2[0].out_features, self.num_classes),\n",
        "                                 nn.Softmax())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv5(self.conv4(self.conv3(self.conv2(self.conv1(x)))))\n",
        "        x = x.reshape(x.shape[0], -1)  # TODO: Try torch.flatten(x, 1)\n",
        "        x = self.fc3(self.fc2(self.fc1(x)))\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def update_size(self, layer, dims=None):\n",
        "        # assuming symmetricity\n",
        "        self.current_size = calc_out_size(in_size=dims if dims else self.current_size, padding=layer.padding, kernel=layer.kernel_size, stride=layer.stride)\n",
        "        print(f'self.current_size = {self.current_size}')\n",
        "\n",
        "\n",
        "model = AlexNet(IMAGE_SHAPE[-1], IMAGE_SHAPE[0], NUM_CLASSES)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Reducing Learning Rate (LR):\n",
        "# In the paper, they reduce LR by 10 each time the validation accuracy stops improving. And it happens 3 times during the 90 epochs.\n",
        "# There are several ways to do this:\n",
        "\n",
        "# 1. Use ReduceLROnPlateau:\n",
        "# scheduler = ReduceLROnPlateau(optimizer, factor=0.10, min_lr=0.0001)\n",
        "# Read more:\n",
        "# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau\n",
        "# https://www.geeksforgeeks.org/adjusting-learning-rate-of-a-neural-network-in-pytorch/\n",
        "# Note though that the docs don't mention how to choose the plateauing metric. You might need to use LightningModule and override\n",
        "# validation_epoch_end():\n",
        "# https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#validation-epoch-level-metrics\n",
        "# https://github.com/PyTorchLightning/pytorch-lightning/issues/1598\n",
        "\n",
        "# 2. Use: MultiStepLR:\n",
        "# Here I picked 3 more or less uniformly distributed milestones, making it deterministic and not dynamic (this ignores loss / acc status).\n",
        "scheduler = MultiStepLR(optimizer, milestones=[25,50,75], gamma=0.10)\n",
        "\n",
        "# 3. Update it manually whenever the validation accuracy stops improving:\n",
        "# for g in optimizer.param_groups:\n",
        "#     g['lr'] /= 10\n",
        "# See:\n",
        "# https://stackoverflow.com/a/48324389/900394\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()  # Paper: \"Our network maximizes the multinomial logistic regression objective\", which is just the cross entropy\n",
        "                                       # according to: https://stats.stackexchange.com/questions/432896/what-is-the-loss-function-used-for-cnn#comment807624_432896\n",
        "training_transformer = transforms.Compose([\n",
        "    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet parameters\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (1.0, 1.0, 1.0)),  # Since the paper mentions normalizng only by subtracting mean, I disabled the STD factor,\n",
        "                                                                   # but one might want to use the actual STD parameters written above.\n",
        "    transforms.Resize(256),  # resize smaller edge to 256\n",
        "    transforms.RandomCrop(IMAGE_SHAPE[-1]),  # Note: In the original paper, they use random cropping to INCREASE the size of the dataset.\n",
        "                                             # We can do this in PyTorch using FiveCrop or TenCrop, but I'm avoiding it here for the sake\n",
        "                                             # of simplicity.\n",
        "                                             # See my answer here for more details:\n",
        "                                             # https://stackoverflow.com/a/68131471/900394\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Note:\n",
        "# According to the paper, \"augmentation\" was also applied on the test set, by 4 corner crops and 2 horizontal reflections, which results\n",
        "# in 10 NEW patches in total per image, and actually INCREASES THE TEST SET SIZE. Then, in the softmax phase, they average the results per patch set.\n",
        "# As mentioned eralier, this can be done using torchvision.transforms.TenCrop, but for the sake of simplicity, I will avoid it here.\n",
        "# Thus, we will simply avoid doing this here.\n",
        "test_transformer = transforms.Compose([\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (1.0, 1.0, 1.0)),  # Although the paper refers only to the training data when talking about normalization,\n",
        "                                                                   # the test data should always be scaled / normalized just as the training data.\n",
        "                                                                   # So they surely do that but it's not mentioned.\n",
        "    transforms.Resize(256),  # resize smaller edge to 256\n",
        "    transforms.CenterCrop(IMAGE_SHAPE[-1]),\n",
        "    # transforms.TenCrop(227),  # disabled\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Note:\n",
        "# I did not apply PCA data augmentation. It can be found here though:\n",
        "# https://github.com/koshian2/PCAColorAugmentation/blob/master/pca_aug_numpy_tensor.py\n",
        "# According to the paper, it reduces the error rate by over 1%. In a nutshell:\n",
        "# To each training image, add the following:\n",
        "# [p1,p2,p3][a1g1,a2g2,a3g3]\n",
        "# where pi and gi are the ith eigenvector and eigenvalue of the 3x3 covariance matrix of the corresponding RGB pixel values,\n",
        "# and ai is a random variable drawn from a Gaussian with mean 0 and std 0.1.\n",
        "# On each epoch, generate a new alpha for each image.\n",
        "# Also read here for adding a custom transform method:\n",
        "# https://discuss.pytorch.org/t/how-to-add-noise-to-mnist-dataset-when-using-pytorch/59745\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "self.current_size = 55\n",
            "self.current_size = 27\n",
            "self.current_size = 27\n",
            "self.current_size = 13\n",
            "self.current_size = 13\n",
            "self.current_size = 13\n",
            "self.current_size = 13\n",
            "self.current_size = 6\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}